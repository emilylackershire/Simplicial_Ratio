{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdf9e046-64ba-4510-973a-64f1269ffda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Always\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from time import time\n",
    "\n",
    "#Sometimes\n",
    "import random\n",
    "import pickle as pkl\n",
    "import igraph as ig\n",
    "import hypernetx as hnx\n",
    "import xgi\n",
    "\n",
    "#My codes\n",
    "import Hypergraph_Models as hm\n",
    "import Hypergraph_Functions as hf\n",
    "import Hypergraph_Processes as hp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3500cf-1d70-4a4a-bb46-6a137d08ec98",
   "metadata": {},
   "source": [
    "# Copying the diffusion function from my other notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7efe40f6-07d4-43bb-a6b1-6f78146e685f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diffusion(w,E,epsilon,slice=1,want='numbers'):\n",
    "    uniform = [sum(w.values())/len(w)]*len(w)\n",
    "    distance = sp.stats.wasserstein_distance(list(w.values()),uniform)\n",
    "    results = {}\n",
    "    if want=='numbers':\n",
    "        results[0] = distance\n",
    "    else:\n",
    "        results[0] = w.copy()\n",
    "    round = 0\n",
    "    if type(epsilon) is int:\n",
    "        for i in range(epsilon-1):\n",
    "            chosen_edges = random.choices(E,k=slice)\n",
    "            for e in chosen_edges:\n",
    "                try:\n",
    "                    new_weight = sum(w[v] for v in set(e))/len(set(e))\n",
    "                except:\n",
    "                    print('found vertex with no weight')\n",
    "                    print(e)\n",
    "                    print(set(e).intersection(set(w.keys())))\n",
    "                for v in set(e):\n",
    "                    w[v] = new_weight\n",
    "            distance = sp.stats.wasserstein_distance(list(w.values()),uniform)\n",
    "            round += slice\n",
    "            if want=='numbers':\n",
    "                results[round] = distance\n",
    "            else:\n",
    "                results[round] = w.copy()\n",
    "    else:\n",
    "        while distance>epsilon:\n",
    "            chosen_edges = random.choices(E,k=slice)\n",
    "            for e in chosen_edges:\n",
    "                new_weight = sum(w[v] for v in set(e))/len(set(e))\n",
    "                for v in set(e):\n",
    "                    w[v] = new_weight\n",
    "            distance = sp.stats.wasserstein_distance(list(w.values()),uniform)\n",
    "            round += slice\n",
    "            if want=='numbers':\n",
    "                results[round] = distance\n",
    "            else:\n",
    "                results[round] = w.copy()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81084e37-0ee7-45d9-8e41-29137c8bf2b2",
   "metadata": {},
   "source": [
    "# Getting and cleaning all of the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c683f802-75d1-491f-b69e-0ed33ccbaa32",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m edge_data \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m filenames:\n\u001b[0;32m---> 17\u001b[0m     H \u001b[38;5;241m=\u001b[39m \u001b[43mxgi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_xgi_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     E \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m([\u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28msorted\u001b[39m(e)) \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m H\u001b[38;5;241m.\u001b[39medges\u001b[38;5;241m.\u001b[39mmembers() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(e)\u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m11\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(e)\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m])) \u001b[38;5;66;03m## changed 11 to 5\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     E \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mset\u001b[39m(e) \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m E]\n",
      "File \u001b[0;32m~/Graph_Env/lib/python3.11/site-packages/xgi/readwrite/xgi_data.py:80\u001b[0m, in \u001b[0;36mload_xgi_data\u001b[0;34m(dataset, cache, read, path, nodetype, edgetype, max_order)\u001b[0m\n\u001b[1;32m     73\u001b[0m         warn(\n\u001b[1;32m     74\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo local copy was found at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcfp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. The data is requested \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     75\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom the xgi-data repository instead. To download a local \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     76\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcopy, use `download_xgi_data`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     77\u001b[0m         )\n\u001b[1;32m     78\u001b[0m data \u001b[38;5;241m=\u001b[39m _request_from_xgi_data(index_url, dataset, cache\u001b[38;5;241m=\u001b[39mcache)\n\u001b[0;32m---> 80\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconvert\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdict_to_hypergraph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnodetype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnodetype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medgetype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medgetype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_order\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_order\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Graph_Env/lib/python3.11/site-packages/xgi/convert/hypergraph_dict.py:84\u001b[0m, in \u001b[0;36mdict_to_hypergraph\u001b[0;34m(data, nodetype, edgetype, max_order)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m edgetype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 84\u001b[0m         edge_data \u001b[38;5;241m=\u001b[39m \u001b[43m{\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mval\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43medge-data\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mH\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medges\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     88\u001b[0m         edge_data \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     89\u001b[0m             edgetype(e): dd\n\u001b[1;32m     90\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m e, dd \u001b[38;5;129;01min\u001b[39;00m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124medge-data\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m     91\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m edgetype(e) \u001b[38;5;129;01min\u001b[39;00m H\u001b[38;5;241m.\u001b[39medges\n\u001b[1;32m     92\u001b[0m         }\n",
      "File \u001b[0;32m~/Graph_Env/lib/python3.11/site-packages/xgi/convert/hypergraph_dict.py:85\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m edgetype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     84\u001b[0m         edge_data \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m---> 85\u001b[0m             key: val \u001b[38;5;28;01mfor\u001b[39;00m key, val \u001b[38;5;129;01min\u001b[39;00m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124medge-data\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m H\u001b[38;5;241m.\u001b[39medges\n\u001b[1;32m     86\u001b[0m         }\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     88\u001b[0m         edge_data \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     89\u001b[0m             edgetype(e): dd\n\u001b[1;32m     90\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m e, dd \u001b[38;5;129;01min\u001b[39;00m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124medge-data\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m     91\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m edgetype(e) \u001b[38;5;129;01min\u001b[39;00m H\u001b[38;5;241m.\u001b[39medges\n\u001b[1;32m     92\u001b[0m         }\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "filenames = [\n",
    "    \"contact-primary-school\", \n",
    "    \"contact-high-school\", \n",
    "    \"hospital-lyon\", \n",
    "    \"email-enron\", \n",
    "    \"email-eu\", \n",
    "    \"diseasome\", \n",
    "    \"disgenenet\", \n",
    "    \"ndc-substances\", \n",
    "    \"congress-bills\", \n",
    "    \"tags-ask-ubuntu\",\n",
    "]\n",
    "datasets = {}\n",
    "vertex_data = {}\n",
    "edge_data = {}\n",
    "for filename in filenames:\n",
    "    H = xgi.load_xgi_data(filename)\n",
    "    E = list(set([tuple(sorted(e)) for e in H.edges.members() if len(e)<=11 and len(e)>=2])) ## changed 11 to 5\n",
    "    E = [set(e) for e in E]\n",
    "    V = list(set([i for e in E for i in e]))\n",
    "    \n",
    "    #For ask-ubuntu, we make two smaller graphs\n",
    "    #One only keeps the first 20000 edges\n",
    "    #The other throws away half of the vertices\n",
    "    if filename == \"tags-ask-ubuntu\": \n",
    "        #Edge-chopped version\n",
    "        V_1 = V.copy()\n",
    "        E_1 = E[:20000]\n",
    "        V_1,E_1 = hf.giant_component(V_1,E_1)\n",
    "        datasets[\"ubuntu (edge-chopped)\"] = (V_1,E_1)\n",
    "        vertex_data[\"ubuntu (edge-chopped)\"] = len(V_1)\n",
    "        edge_data[\"ubuntu (edge-chopped)\"] = len(E_1)\n",
    "\n",
    "        #Vertex-chopped version\n",
    "        V_2 = []\n",
    "        for v in V:\n",
    "            if random.choice([1,2])==1:\n",
    "                V_2.append(v)\n",
    "        #We use the strict definition of induced subgraph\n",
    "        E_2 = []\n",
    "        for e in E:\n",
    "            accept = True\n",
    "            for v in e:\n",
    "                if v not in V_2:\n",
    "                    accept = False\n",
    "                    break\n",
    "            if accept:\n",
    "                E_2.append(e)\n",
    "        V_2,E_2 = hf.giant_component(V_2,E_2)\n",
    "        datasets[\"ubuntu (vertex-chopped)\"] = (V_2,E_2)\n",
    "        vertex_data[\"ubuntu (vertex-chopped)\"] = len(V_2)\n",
    "        edge_data[\"ubuntu (vertex-chopped)\"] = len(E_2)\n",
    "    #All other datasets are built normally\n",
    "    else:\n",
    "        V,E = hf.giant_component(V,E)\n",
    "        datasets[filename] = (V,E)\n",
    "        vertex_data[filename] = len(V)\n",
    "        edge_data[filename] = len(E)\n",
    "\n",
    "df = pd.DataFrame({'n': vertex_data.copy(), 'm': edge_data.copy()})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2766df55-c707-4b63-b954-f61768f60433",
   "metadata": {},
   "source": [
    "Verifying that all of the datasets are built correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44e9ce6-8645-4eda-89d2-b369c79c27a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in datasets.keys():\n",
    "    print(name)\n",
    "    print('|V| = {0}, |E| = {1}\\n'.format(len(datasets[name][0]),len(datasets[name][1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6530bbd-1d59-467d-94f3-144ebc1ac224",
   "metadata": {},
   "source": [
    "# Setting parameters for all experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8803a63-2c89-437a-b4e0-5f8f89acca14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Whether or not we include multisets should be consistent across all experiments\n",
    "multisets = False\n",
    "skeleton = True\n",
    "\n",
    "#The number of samples for each experiment will be different\n",
    "#For example: adversarial growth takes forever so should only be run a few times\n",
    "rolls = {\n",
    "    'random growth': 100,\n",
    "    'adversarial growth': 20,\n",
    "    'single-source diffusion': 100,\n",
    "    '10% sprinkled diffusion': 100,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddbd50b-c887-4c28-af26-772838996585",
   "metadata": {},
   "source": [
    "# Getting the fitted parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bdd8b77-0a30-4c3c-bf05-d8e61735d6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {}\n",
    "m = {}\n",
    "q_list = [0, 0.5, 1]\n",
    "\n",
    "for name in datasets.keys():\n",
    "    V,E = datasets[name]\n",
    "\n",
    "    #####\n",
    "    # d #\n",
    "    #####\n",
    "    d[name] = hf.degrees(V,E)\n",
    "\n",
    "    #####\n",
    "    # m #\n",
    "    #####\n",
    "    E_sorted = hf.sort_edges(E)\n",
    "    m[name] = {}\n",
    "    for k in E_sorted.keys():\n",
    "        m[name][k] = len(E_sorted[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7b6b7df-d496-47e5-a444-8fa0c3e4cbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_start = time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bc8f02-14c5-450b-90af-c49ba862d6af",
   "metadata": {},
   "source": [
    "# Experiment 1: random growth\n",
    "We start at round 0 with no edges and a max component size of 1\n",
    "\n",
    "On round i+1, we choose a random edge (not already added) and add it to the graph, then track the max component size\n",
    "\n",
    "We save the data as the dictionary (round -> size of giant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed32c93-06f6-43f8-9079-00886386eb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is what will get added to the dataframe\n",
    "experiment = {}\n",
    "\n",
    "#Parameters were set at the beginning\n",
    "num_rolls = rolls['random growth']\n",
    "\n",
    "\n",
    "limited_names = [\n",
    "    \"hospital-lyon\", \n",
    "    \"disgenenet\", \n",
    "]\n",
    "\n",
    "for name in datasets.keys():\n",
    "    V,E = datasets[name]\n",
    "    results = hp.giant_component_growth(V, E)\n",
    "    for i in range(num_rolls - 1):\n",
    "        sample = hp.giant_component_growth(V, E)\n",
    "        for j in results.keys():\n",
    "            results[j] += sample[j]\n",
    "    for j in results.keys():\n",
    "        results[j] = results[j]/num_rolls\n",
    "    experiment[name] = results.copy()\n",
    "\n",
    "    for q in q_list: \n",
    "        E = hm.simplicial_chung_lu(d[name], m[name], q, multisets=multisets, skeleton=skeleton)\n",
    "        results = hp.giant_component_growth(V,E)\n",
    "        for i in range(num_rolls-1):\n",
    "            E = hm.simplicial_chung_lu(d[name], m[name], q, multisets=multisets, skeleton=skeleton)\n",
    "            sample = hp.giant_component_growth(V,E)\n",
    "            for j in results.keys():\n",
    "                results[j] += sample[j]\n",
    "        for j in results.keys():\n",
    "            results[j] = results[j]/num_rolls\n",
    "    \n",
    "        #Saving the results\n",
    "        experiment['{0} (q={1})'.format(name, q)] = results.copy()\n",
    "\n",
    "#Saving to the dataframe\n",
    "df_add = pd.DataFrame({'random growth':experiment.copy()})\n",
    "df = pd.concat([df,df_add],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bceddcd2-12d9-43bc-914d-3e11fe85f394",
   "metadata": {},
   "source": [
    "Verifying that the experiment was compiled correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb95f3d-3a93-46f1-9648-0b42da94e8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf8681d-5132-45df-8fe1-fa1df3befd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('fixed_experiments.pkl','wb') as file:\n",
    "    pkl.dump(df, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e2e339-a11f-4904-b3cf-1c431e65268f",
   "metadata": {},
   "source": [
    "# Experiment 2: adversarial growth\n",
    "From the input graph, we compute the betweenness value of each edge\n",
    "\n",
    "We start at round 0 with no edges and a max component size of 1\n",
    "\n",
    "At round i+1, we pick the edge with smallest betweenness from the set of edges not chosen yet and add it to the graph, tracking the size of the giant\n",
    "\n",
    "We save the data as the dictionary (round -> size of giant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e15f67-52bf-4c7a-babb-18d1a10fa586",
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "#Francois' speed-up, (thanks for this!)\n",
    "##\n",
    "\n",
    "#Simple function to reorder edges by betweenness\n",
    "#Smallest to largest\n",
    "def reorder_by_betweenness_fast(edges):\n",
    "    E = edges.copy()\n",
    "    G = hnx.Hypergraph(E)\n",
    "    LG = G.get_linegraph()\n",
    "    g = ig.Graph.from_networkx(LG)\n",
    "    bet_ig = g.betweenness()\n",
    "    n = g.vcount()\n",
    "    norm = 2/((n-1)*(n-2))\n",
    "    betweenness = dict(zip(g.vs['_nx_name'],[x*norm for x in bet_ig])) \n",
    "    E_arranged = list(sorted(betweenness, key=betweenness.get))\n",
    "    E = [E[i] for i in E_arranged]\n",
    "    return E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d68a1d-b363-417f-ba71-dc498d858ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "#Now the experiment\n",
    "##\n",
    "\n",
    "#This is what gets added to the dataframe\n",
    "#Note that I save a copy of 'experiment' to the dataframe, so re-initializing it is fine\n",
    "experiment = {}\n",
    "\n",
    "#Parameters were set at the beginning\n",
    "num_rolls = rolls['adversarial growth']\n",
    "\n",
    "for name in limited_names:\n",
    "    V,E = datasets[name]\n",
    "    E = reorder_by_betweenness_fast(E)\n",
    "    results = hp.giant_component_growth(V, E, shuffle_edges=False)\n",
    "    experiment[name] = results.copy()\n",
    "\n",
    "    for q in q_list: \n",
    "        E = hm.simplicial_chung_lu(d[name], m[name], q, multisets=multisets, skeleton=skeleton)\n",
    "        E = reorder_by_betweenness_fast(E)\n",
    "        results = hp.giant_component_growth(V, E, shuffle_edges=False)\n",
    "        for i in range(num_rolls-1):\n",
    "            E = hm.simplicial_chung_lu(d[name], m[name], q, multisets=multisets, skeleton=skeleton)\n",
    "            E = reorder_by_betweenness_fast(E)\n",
    "            sample = hp.giant_component_growth(V, E, shuffle_edges=False)\n",
    "            for j in results.keys():\n",
    "                results[j] += sample[j]\n",
    "        for j in results.keys():\n",
    "            results[j] = results[j]/num_rolls\n",
    "    \n",
    "        #Saving the results\n",
    "        experiment['{0} (q={1})'.format(name, q)] = results.copy()\n",
    "    \n",
    "    \n",
    "#Saving to the dataframe\n",
    "df_add = pd.DataFrame({'adversarial growth':experiment.copy()})\n",
    "df = pd.concat([df,df_add],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f28b11-cb10-4866-a850-8c135bada167",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sanity check\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961b3d54-3bdf-420e-bec5-442162f8e123",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('fixed_experiments.pkl','wb') as file:\n",
    "    pkl.dump(df, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7db23c-6457-4085-a22a-4a3f3f8b9e77",
   "metadata": {},
   "source": [
    "# Experiment 3: Single-source diffusion\n",
    "We start with a weight function with w(v) = 0 for all but one vertex; a random vertex gets w(v) = 1\n",
    "\n",
    "Each round, we pick an edge and replace the vertex weights by the average\n",
    "\n",
    "We save the dictionary (round -> wasserstein distance between w and uniform)\n",
    "\n",
    "The first time we run the experiment, we run until the distance is < 1/(20|V|) and we record the number of rounds this took\n",
    "\n",
    "Then we run every other sample for the recorded number of rounds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10a9a1e-37a0-43ae-9d49-13b6972fae66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is what gets added to the dataframe\n",
    "#Note that I save a copy of 'experiment' to the dataframe, so re-initializing it is fine\n",
    "experiment = {}\n",
    "        \n",
    "#Parameters were set at the beginning\n",
    "num_rolls = rolls['single-source diffusion']\n",
    "\n",
    "for name in datasets.keys():\n",
    "    V, E = datasets[name]\n",
    "\n",
    "    #s determines how often the wasserstein distance is computed\n",
    "    s = 10\n",
    "\n",
    "    #Weight function\n",
    "    w = {v: 0 for v in V}\n",
    "    #a random node gets weight 1\n",
    "    w[random.choice(V)] = 1   \n",
    "    results = diffusion(w, E, 1/(20*len(V)), slice=s)\n",
    "\n",
    "    #Record the length of the experiment and fix it for the rest\n",
    "    first_len = len(results)\n",
    "    for i in range(num_rolls-1):\n",
    "        w = {v:0 for v in V}\n",
    "        w[random.choice(V)] = 1\n",
    "        sample = diffusion(w, E, first_len, slice=s)\n",
    "        for j in results.keys():\n",
    "            results[j] += sample[j]\n",
    "    for j in results.keys():\n",
    "        results[j] = results[j]/num_rolls\n",
    "\n",
    "    #Saving\n",
    "    experiment[name] = results.copy()\n",
    "\n",
    "\n",
    "    for q in q_list:\n",
    "        E_bu = hm.simplicial_chung_lu(d[name], m[name], q, multisets=multisets, skeleton=skeleton)\n",
    "        \n",
    "        w = {v: 0 for v in V}\n",
    "        w[random.choice(V)] = 1   \n",
    "        results = diffusion(w, E_bu, first_len, slice=s)\n",
    "        for i in range(num_rolls-1):\n",
    "            E_bu = hm.simplicial_chung_lu(d[name], m[name], q, multisets=multisets, skeleton=skeleton)\n",
    "            w = {v:0 for v in V}\n",
    "            w[random.choice(V)] = 1\n",
    "            sample = diffusion(w, E_bu, first_len, slice=s)\n",
    "            for j in results.keys():\n",
    "                results[j] += sample[j]\n",
    "        for j in results.keys():\n",
    "            results[j] = results[j]/num_rolls\n",
    "    \n",
    "        #Saving\n",
    "        experiment['{0} (q={1})'.format(name, q)] = results.copy()\n",
    "    \n",
    "#Saving to the dataframe\n",
    "df_add = pd.DataFrame({'single-source diffusion':experiment.copy()})\n",
    "df = pd.concat([df,df_add],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef680775-76ad-40f0-89bf-dc9e4d7864b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sanity check\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f42cd5-390c-4e7a-a57e-26cf256a750e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('fixed_experiments.pkl','wb') as file:\n",
    "    pkl.dump(df, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521c5aef-325f-498c-a799-6bdf59c30cee",
   "metadata": {},
   "source": [
    "# Experiment 4: 10% sprinkled diffusion\n",
    "Identical to experiment 3 but 10% of nodes start with w(v) = 1 and we stop when the distance is < 0.005\n",
    "\n",
    "In both of these experiments, the idea is to reach 5% of the initial distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c65b5d5-94ab-4087-9a9d-b1b1dddc2d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is what gets added to the dataframe\n",
    "#Note that I save a copy of 'experiment' to the dataframe, so re-initializing it is fine\n",
    "experiment = {}\n",
    "\n",
    "#Parameters were set at the beginning\n",
    "num_rolls = rolls['10% sprinkled diffusion']\n",
    "\n",
    "for name in datasets.keys():\n",
    "    V,E = datasets[name]\n",
    "\n",
    "    #s determines how often the wasserstein distance is computed\n",
    "    s = 10\n",
    "\n",
    "    #Weight function\n",
    "    w = {v:0 for v in V}\n",
    "    #10% of nodes get re-weighted\n",
    "    V_ini = random.sample(V,k=round(len(V)/10))\n",
    "    for v in V_ini:\n",
    "        w[v] = 1 \n",
    "    results = diffusion(w,E,0.005,slice=s)\n",
    "\n",
    "    #Record the length of the experiment and fix it for the rest\n",
    "    first_len = len(results)\n",
    "    \n",
    "    for i in range(num_rolls-1):\n",
    "        w = {v:0 for v in V}\n",
    "        V_ini = random.sample(V,k=round(len(V)/10))\n",
    "        for v in V_ini:\n",
    "            w[v] = 1 \n",
    "        sample = diffusion(w,E,first_len,slice=s)\n",
    "        for j in results.keys():\n",
    "            results[j] += sample[j]\n",
    "    for j in results.keys():\n",
    "        results[j] = results[j]/num_rolls\n",
    "\n",
    "    #Saving\n",
    "    experiment[name] = results.copy()\n",
    "\n",
    "    for q in q_list:\n",
    "        #Bottom-up\n",
    "        E_bu = hm.simplicial_chung_lu(d[name], m[name], q, multisets=multisets, skeleton=skeleton)\n",
    "        \n",
    "        #Weight function\n",
    "        w = {v:0 for v in V}\n",
    "        #10% of nodes get re-weighted\n",
    "        V_ini = random.sample(V,k=round(len(V)/10))\n",
    "        for v in V_ini:\n",
    "            w[v] = 1 \n",
    "        results = diffusion(w, E_bu, first_len, slice=s)\n",
    "        for i in range(num_rolls-1):\n",
    "            E_bu = hm.simplicial_chung_lu(d[name], m[name], q, multisets=multisets, skeleton=skeleton)\n",
    "            #Weight function\n",
    "            w = {v:0 for v in V}\n",
    "            #10% of nodes get re-weighted\n",
    "            V_ini = random.sample(V,k=round(len(V)/10))\n",
    "            for v in V_ini:\n",
    "                w[v] = 1 \n",
    "            sample = diffusion(w, E_bu, first_len, slice=s)\n",
    "            for j in results.keys():\n",
    "                results[j] += sample[j]\n",
    "        for j in results.keys():\n",
    "            results[j] = results[j]/num_rolls\n",
    "    \n",
    "        #Saving\n",
    "        experiment['{0} (q={1})'.format(name, q)] = results.copy()\n",
    "    \n",
    "#Saving to the dataframe\n",
    "df_add = pd.DataFrame({'10% sprinkled diffusion':experiment.copy()})\n",
    "df = pd.concat([df,df_add],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfa4bfb-9f2e-416c-8735-03ba70c92bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sanity check\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9799101-a60d-40b4-9840-109967854b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('fixed_experiments.pkl','wb') as file:\n",
    "    pkl.dump(df, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6710e8-17e8-4a57-baa9-4abcad82d86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('fixed_experiments.pkl','rb') as file:\n",
    "    df = pkl.load(file)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f113db1-4557-493e-8e0d-a5900dce5b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_end = time()\n",
    "print((t_end - t_start)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3898d72e-644a-4292-bfc5-9b4880cd2117",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Graph environment",
   "language": "python",
   "name": "graph_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
